# SWE-bench Comparison Framework Environment Variables
# ====================================================
# Copy this file to .env and fill in your API keys

# ============================================
# LLM API Keys (at least one is required)
# ============================================

# OpenAI API Key (for GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic API (for Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Azure OpenAI (if using Azure)
AZURE_OPENAI_API_KEY=your_azure_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2023-12-01-preview

# Google AI (for Gemini models)
GOOGLE_API_KEY=your_google_api_key_here

# Together AI (for open-source models)
TOGETHER_API_KEY=your_together_api_key_here

# Hugging Face (for model downloads)
HUGGINGFACE_TOKEN=your_hf_token_here

# ============================================
# Experiment Configuration
# ============================================

# Default output directory for results
SWE_OUTPUT_DIR=./results

# Default repository cache directory
SWE_REPOS_DIR=./repos

# Default index cache directory
SWE_INDEXES_DIR=./indexes

# Default log level (DEBUG, INFO, WARNING, ERROR)
SWE_LOG_LEVEL=INFO

# ============================================
# Execution Configuration
# ============================================

# Maximum parallel workers
SWE_MAX_WORKERS=4

# Timeout per instance (seconds)
SWE_TIMEOUT=300

# Enable Docker sandbox
SWE_USE_DOCKER=false

# Docker image for sandbox
SWE_DOCKER_IMAGE=swe-bench/sandbox:latest

# ============================================
# Model Configuration
# ============================================

# Default model
SWE_DEFAULT_MODEL=gpt-5-mini

# Model temperature
SWE_MODEL_TEMPERATURE=0.0

# Maximum tokens per request
SWE_MAX_TOKENS=4096

# Rate limiting (requests per minute)
SWE_RATE_LIMIT=60

# ============================================
# RAG Configuration
# ============================================

# Default embedding model
SWE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Default reranking model
SWE_RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Default chunk size
SWE_CHUNK_SIZE=1000

# Default chunk overlap
SWE_CHUNK_OVERLAP=200

# Default top-k for retrieval
SWE_TOP_K=10

# ============================================
# Experiment Tracking (Optional)
# ============================================

# MLflow
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=swe-bench-comparison

# Weights & Biases
WANDB_PROJECT=swe-bench-comparison
WANDB_ENTITY=your-entity
WANDB_API_KEY=your_wandb_key_here

# ============================================
# Development Settings
# ============================================

# Enable debug mode
DEBUG=false

# Enable caching of LLM responses
CACHE_LLM_RESPONSES=true

# Cache directory
CACHE_DIR=./cache

# Force reindex repositories
FORCE_REINDEX=false

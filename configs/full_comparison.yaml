# Full Comparison Configuration
# Compares all methods on SWE-bench Verified dataset

experiment_name: "full_comparison"
output_dir: "./results/full_comparison"

# Dataset configuration
dataset: "verified"  # Full verified dataset for comprehensive comparison

# Repository configuration
repos_dir: "./repos"

# Sandbox configuration (use Docker for full evaluation)
sandbox: "docker"

# Parallel execution
max_workers: 4

# Methods to compare
methods:
  # ============================================
  # Agentic Methods
  # ============================================
  
  # AutoCodeRover: Agentic with structured exploration
  autocoderover:
    type: "agentic"
    strategy: "autocoderover"
    model: "gpt-5-mini"
    model_config:
      temperature: 1.0
      max_tokens: 4096
    agentic_config:
      max_iterations: 15
      search_depth: 5
      tools:
        - "search"
        - "view"
        - "edit"
        - "test"
        - "debug"
      exploration_strategy: "structured"
      use_oracle: false
  
  # SWE-agent: Full agent-based approach
  swe_agent:
    type: "agentic"
    strategy: "swe_agent"
    model: "gpt-5-mini"
    model_config:
      temperature: 1.0
      max_tokens: 4096
    agentic_config:
      max_iterations: 50
      search_depth: 10
      tools:
        - "search"
        - "view"
        - "edit"
        - "test"
        - "debug"
        - "shell"
      exploration_strategy: "reactive"
      use_history: true
  
  # Agentless: Agentic without explicit agent
  agentless:
    type: "agentic"
    strategy: "agentless"
    model: "gpt-5-mini"
    model_config:
      temperature: 1.0
      max_tokens: 4096
    agentic_config:
      max_iterations: 10
      search_depth: 3
      tools:
        - "search"
        - "view"
        - "edit"
        - "test"
      exploration_strategy: "breadth_first"
  
  # ============================================
  # RAG Methods
  # ============================================
  
  # BM25 RAG: Traditional keyword-based retrieval
  bm25_rag:
    type: "rag"
    retrieval: "bm25"
    model: "gpt-5-mini"
    model_config:
      temperature: 1.0
      max_tokens: 4096
    rag_config:
      top_k: 15
      chunk_size: 1000
      chunk_overlap: 200
      use_query_expansion: true
  
  # Dense RAG: Neural embedding-based retrieval
  dense_rag:
    type: "rag"
    retrieval: "dense"
    model: "gpt-5-mini"
    model_config:
      temperature: 1.0
      max_tokens: 4096
    rag_config:
      top_k: 15
      chunk_size: 1000
      chunk_overlap: 200
      embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
      use_query_expansion: true
  
  # Hybrid RAG: Combines BM25 and dense retrieval
  hybrid_rag:
    type: "rag"
    retrieval: "hybrid"
    model: "gpt-5-mini"
    model_config:
      temperature: 1.0
      max_tokens: 4096
    rag_config:
      top_k: 15
      chunk_size: 1000
      chunk_overlap: 200
      embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
      bm25_weight: 0.3
      dense_weight: 0.7
      rerank: true
      rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      use_query_expansion: true

# ============================================
# Alternative Model Configurations
# ============================================

# Uncomment to run with Claude
# claude_hybrid_rag:
#   type: "rag"
#   retrieval: "hybrid"
#   model: "claude-4.5-sonnet"
#   model_config:
#     temperature: 0.0
#     max_tokens: 4096
#   rag_config:
#     top_k: 10

# Evaluation configuration
evaluation:
  metrics:
    - "resolution"
    - "pass_at_k"
    - "localization"
    - "codebleu"
    - "token_usage"
    - "semantic_entropy"
  pass_at_k_values: [1, 5, 10]
  timeout_per_instance: 300  # seconds

# Reporting configuration
report:
  formats:
    - "json"
    - "markdown"
    - "html"
    - "csv"
  include_visualizations: true
  include_code_diffs: true
  include_per_instance_results: true

# Logging configuration
logging:
  level: "INFO"
  save_logs: true
  log_file: "experiment.log"

# Experiment tracking (optional)
tracking:
  enabled: true
  backend: "mlflow"  # or "wandb"
  experiment_name: "full_comparison"  # Should match experiment_name above
  uri: "http://localhost:5000"  # MLflow tracking URI
  # For Weights & Biases, use:
  # backend: "wandb"
  # wandb:
  #   project: "swe-bench-comparison"
  #   entity: "your-entity"
